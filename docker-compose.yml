services:
  # Mailtagger - Gmail categorizer
  mailtagger:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: mailtagger-app
    restart: unless-stopped
    network_mode: host
    environment:
      # LLM Provider Configuration
      # Using local Ollama instance running on host (via network_mode: host)
      - LLM_PROVIDER=ollama
      - OLLAMA_URL=http://localhost:11434/v1/chat/completions
      - OLLAMA_MODEL=llama3.1:8b
      
      # OpenAI Configuration (alternative)
      # - LLM_PROVIDER=openai
      # - OPENAI_API_KEY=${OPENAI_API_KEY}
      # - OPENAI_MODEL=gpt-4o-mini
      
      # Gmail Labels
      - LABEL_ECOMMERCE=AI_Ecommerce
      - LABEL_POLITICAL=AI_Political
      - LABEL_TRIAGED=AI_Triaged
      
      # Gmail Query
      - GMAIL_QUERY=in:inbox newer_than:14d -label:AI_Triaged
      
      # Processing Settings
      - MAX_RESULTS=40
      - SLEEP_SECONDS=0.5
      
      # Daemon Settings
      - DAEMON_INTERVAL=300
      - CREDENTIALS_PATH=/app/data
      
      # Logging
      - LOG_LEVEL=INFO
      
      # Retry Settings
      - MAX_RETRIES=3
      - RETRY_BACKOFF=2.0
      - OPENAI_TIMEOUT=45
    
    volumes:
      # Mount credentials directory
      # IMPORTANT: Place credentials.json and token.json in ./data/ before starting
      - ./data:/app/data:rw
    
    healthcheck:
      test: ["CMD", "python3", "-c", "import sys; sys.exit(0)"]
      interval: 60s
      timeout: 10s
      retries: 3
      start_period: 60s
    
    # Uncomment to override command (e.g., for different interval)
    # command: ["python3", "gmail_categorizer.py", "--daemon", "--credentials-path", "/app/data", "--interval", "600"]

  # Prompt Management API
  prompt-api:
    build:
      context: .
      dockerfile: Dockerfile.api
    container_name: mailtagger-api
    restart: unless-stopped
    network_mode: host
    environment:
      - PROMPT_DB_PATH=/app/data/prompts.db
      - GMAIL_CREDENTIALS_PATH=/app/data
      - HOST=0.0.0.0
      - PORT=8000
      # LLM Configuration (for testing)
      # Using local Ollama instance running on host (via network_mode: host)
      - LLM_PROVIDER=ollama
      - OLLAMA_URL=http://localhost:11434/v1/chat/completions
      - OLLAMA_MODEL=llama3.1:8b
      - CREDENTIALS_PATH=/app/data
    volumes:
      - ./data:/app/data:rw
    healthcheck:
      test: ["CMD", "python3", "-c", "import requests; requests.get('http://localhost:8000/health').raise_for_status()"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s

  # Prompt Management Web UI
  prompt-ui:
    image: nginx:alpine
    container_name: mailtagger-ui
    restart: unless-stopped
    # Note: depends_on removed because prompt-api uses network_mode: host
    # The UI will connect to the API via browser (localhost:8000)
    ports:
      - "8080:80"
    volumes:
      - ./web:/usr/share/nginx/html:ro
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost/"]
      interval: 30s
      timeout: 10s
      retries: 3
